{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EECS 484 Final Project Chaitanya Kolluru.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"FeIDZ83N1ILj","colab_type":"text"},"cell_type":"markdown","source":["*U-net to segment endothelial cells in specular microscopy images*\n","\n","Include libraries"]},{"metadata":{"id":"0CCRwXuL1US2","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import random as rn\n","import glob\n","import re"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PbuhmvE76mVZ","colab_type":"text"},"cell_type":"markdown","source":["Configure and start a tensorflow session"]},{"metadata":{"id":"4r2M2fIa6qB7","colab_type":"code","colab":{}},"cell_type":"code","source":["session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Hq1Sfiq96xTB","colab_type":"text"},"cell_type":"markdown","source":["Import keras specific classes"]},{"metadata":{"id":"EZagbtZn64lr","colab_type":"code","outputId":"22fcef8f-6785-4f11-c74c-0ae4398ea09f","executionInfo":{"status":"ok","timestamp":1544480168076,"user_tz":300,"elapsed":379,"user":{"displayName":"Chaitanya Kolluru","photoUrl":"https://lh6.googleusercontent.com/-MQKYUBE5RV0/AAAAAAAAAAI/AAAAAAAAAC4/izr2wv46QEg/s64/photo.jpg","userId":"12376537687734641634"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from keras.models import *\n","from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, concatenate\n","from keras.optimizers import *\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras import backend as keras\n","import argparse\n","from keras.utils import plot_model\n","import keras\n","from keras import backend as K\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"5WfSFY6g6-rz","colab_type":"text"},"cell_type":"markdown","source":["Set the session in keras"]},{"metadata":{"id":"LC_zXU1G7Dib","colab_type":"code","outputId":"5ed86d63-f094-40c0-e4f8-bc18de688d85","executionInfo":{"status":"ok","timestamp":1544480170485,"user_tz":300,"elapsed":267,"user":{"displayName":"Chaitanya Kolluru","photoUrl":"https://lh6.googleusercontent.com/-MQKYUBE5RV0/AAAAAAAAAAI/AAAAAAAAAC4/izr2wv46QEg/s64/photo.jpg","userId":"12376537687734641634"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["K.set_session(sess)\n","print (keras.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2.2.4\n"],"name":"stdout"}]},{"metadata":{"id":"1jU2i4LVB95s","colab_type":"text"},"cell_type":"markdown","source":["Mount google drive to access training and testing images and labels"]},{"metadata":{"id":"jDjATIofLnX0","colab_type":"code","outputId":"c16c9090-2db0-4406-dae9-0c43126aefa7","executionInfo":{"status":"ok","timestamp":1544480207656,"user_tz":300,"elapsed":36071,"user":{"displayName":"Chaitanya Kolluru","photoUrl":"https://lh6.googleusercontent.com/-MQKYUBE5RV0/AAAAAAAAAAI/AAAAAAAAAC4/izr2wv46QEg/s64/photo.jpg","userId":"12376537687734641634"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"cell_type":"code","source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"QldY8wC3MgYo","colab_type":"text"},"cell_type":"markdown","source":["Define the loss function to train the neural network\n","\n","We will use a weighted form the cross entropy loss function\n","\n","(Less commonly seen classes will have heigher weights)\n","\n","Cell border pixels are weighted nearly 4 times more as compared to other pixels"]},{"metadata":{"id":"mImpX2aIMrKP","colab_type":"code","colab":{}},"cell_type":"code","source":["def weighted_binary_crossentropy(y_true, y_pred):\n","\n","\ty_true_f = K.flatten(y_true)\n","\ty_pred_f = K.flatten(y_pred)\n","\n","\tbinary_crossentropy = K.binary_crossentropy(y_true_f, y_pred_f)\n","\tweighted_vector = y_true_f * 0.21 + (1. - y_true_f) * 0.79\n","\tweighted_binary_crossentropy_loss = weighted_vector * binary_crossentropy\n","\n","\treturn K.mean(weighted_binary_crossentropy_loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"91MPXaiJRm-8","colab_type":"text"},"cell_type":"markdown","source":["Helper function to sort images in the right order\n","\n","For example, 1.bmp, 2.bmp, 3.bmp instead of 1.bmp, 10.bmp, 11.bmp etc."]},{"metadata":{"id":"WUqUeMuLRxIH","colab_type":"code","colab":{}},"cell_type":"code","source":["def natural_keys(text):\n","\n","    # Assumes that the path is of the form /content/.../1.jpg\n","    c = re.split('(/\\d+)', text)\n","    print(c)\n","    return int(c[1].split('/')[1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EP9e2YiP7M43","colab_type":"text"},"cell_type":"markdown","source":["Define the U-Net class, will contain routines to read/write data, train/test networks\n","\n","Object to this class will be instantiated in __main__\n","\n","Specify location to the training and testing data in data_dir variable\n","\n","__load_data__ reads the training images, labels and testing images from the google drive\n","\n","__get_unet__ specifies the network architecture for U-net\n","\n","__train_and_test__ trains and tests the neural network"]},{"metadata":{"id":"t6qp6Lkf7f5f","colab_type":"code","outputId":"bfee7aeb-c755-4cb8-a98e-1d1c4a46982a","colab":{"base_uri":"https://localhost:8080/","height":3276}},"cell_type":"code","source":["class myUnet(object):\n","\n","    def __init__(self):\n","\n","        self.data_dir = '/content/drive/My Drive/Colab Notebooks/'\n","    \n","    def load_data(self):\n","\n","        # Augment the training images and labels, because 34 training images is a small number of images for deep learning methods\n","        datagen_args = dict(rotation_range=0.2,\n","                            width_shift_range=0.05,\n","                            height_shift_range=0.05,\n","                            shear_range= 0.05,\n","                            zoom_range=0.05,\n","                            horizontal_flip=True,\n","                            fill_mode='nearest',\n","                            rescale=1./255)\n","        \n","        image_datagen = ImageDataGenerator(**datagen_args)\n","        labels_datagen = ImageDataGenerator(**datagen_args)\n","\n","        \n","        # Load training images and labels from the respective directories\n","        image_generator = image_datagen.flow_from_directory(self.data_dir + 'data/train/image/', color_mode='grayscale', class_mode=None, seed=1, batch_size=1, target_size=(480, 320))\n","        labels_generator = labels_datagen.flow_from_directory(self.data_dir + 'data/train/label/', color_mode='grayscale', class_mode=None, seed=1, batch_size=1, target_size=(480, 320))\n","        \n","        # Find all test images from the data folder\n","        self.test_imgs_list = glob.glob(self.data_dir + 'test/image/*.bmp')\n","\n","        # Sort so that the list is 1.jpg, 2.jpg etc. and not 1.jpg, 11.jpg etc.\n","        self.test_imgs_list.sort(key=natural_keys)\n","\n","        # Read test image files and load them into a numpy array\n","        imgs_test_stack = np.zeros((len(self.test_imgs_list), 480, 320))\n","        for i in np.arange(len(self.test_imgs_list)):\n","            imgs_test_stack[i,:,:] = img_to_array(load_img(self.test_imgs_list[i]))[:,:,1]/255\n","        \n","        print('%d EC test images found' %(len(self.test_imgs_list)))\n","        \n","        #adds a dimension of 1 at the end\n","        imgs_test_stack = np.expand_dims(imgs_test_stack, axis=-1)\n","\n","        return image_generator, labels_generator, imgs_test_stack\n","      \n","    def get_unet(self):\n","\n","        # Neural network architecture\n","        # Based on the U-Net described here: https://arxiv.org/abs/1505.04597\n","        \n","        # Input layer shape\n","        inputs = Input((480, 320, 1))        \n","\n","        # First Encoding Block\n","        # Convolutional layers of 128 filters each, learning 3x3 kernel\n","        # ReLU activation. Kernel weights initialized as 2/sqrt(fan_in), \n","        # where fan_in is the number of units feeding into this network.\n","        # Maximum pooling \"pools\" a 2x2 area and picks the maximum value\n","        # Gives some form of spatial invariance\n","        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","        # Second Encoding block\n","        # Same as previous, but with more filters learnt (256 vs. 128)\n","        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","        # Third Encoding Block\n","        # Dropout (connections are randomly dropped in the network)\n","        # Proportion of connections dropped is 0.5\n","        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","        drop4 = Dropout(0.5)(conv4)\n","        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","        # Fourth Encoding Block\n","        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","        drop5 = Dropout(0.5)(conv5)\n","\n","        # First Decoding (Upsampling) Block\n","        # Transposed convolutions with kernel size (2,2)\n","        # Subsequent concatenation from lower encoding block outputs\n","        # Two convolutional layers\n","        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","        merge6 = concatenate([drop4, up6], axis=3)\n","        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","\n","        # Second Decoding Block\n","        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","        merge7 = concatenate([conv3, up7], axis=3)\n","        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","        # Third Decoding Block\n","        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","        merge8 = concatenate([conv2, up8], axis=3)\n","        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","        conv8 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","        conv9 = Conv2D(1, 1, activation = 'sigmoid')(conv8)\n","\n","        # Define the inputs and outputs to the network\n","        model = Model(inputs = inputs, outputs = conv9)\n","\n","        # Use the weighted cross entropy loss function as described above\n","        # Adam optimizer is used with a learning rate of 1e-4\n","        model.compile(optimizer = Adam(lr = 1e-4), loss = weighted_binary_crossentropy,  metrics = ['accuracy'])\n","\n","        model.summary()\n","\n","        return model\n","\n","    def train_and_test(self):\n","\n","            print('-' * 30)\n","            print('Training on EC microscopy images')\n","            print('-' * 30)\n","\n","            print('Loading training data (EC cells in microscopy images)')\n","            imgs_train, imgs_train_labels, imgs_test = self.load_data()\n","            print('Loaded training data (EC cells in microscopy images) \\n')\n","\n","            # Get the neural network architecture\n","            print('Loading network architecture')\n","            model = self.get_unet()\n","            print('Loaded network architecture \\n')\n","\n","            # Create a checkpoint to save the network weights to a file. Loss on the validation set will be monitored\n","            model_checkpoint = ModelCheckpoint(self.data_dir + 'unet_train.hdf5', monitor='loss',verbose=1, save_best_only=True)\n","\n","            # Fit the model to the training data\n","            print('Fitting model to train dataset')\n","            model.fit_generator(zip(imgs_train, imgs_train_labels), steps_per_epoch=32, epochs=5, verbose=1, shuffle=True, callbacks=[model_checkpoint])\n","\n","            print('-' * 30)\n","            print('Test on unseen EC microscopy images')\n","            print('-' * 30)\n","\n","            # Load the network weights\n","            print('Loading network weights from unet_train.hdf5 file')\n","            model.load_weights(self.data_dir + 'unet_train.hdf5')\n","            print('Loaded weights from the pre-trained network \\n')\n","\n","            # Predict on the test images\n","            imgs_test_predictions = model.predict(imgs_test, batch_size=1, verbose=1)\n","            print('Predicted on test EC images')\n","\n","            # Save predictions to the results folder\n","            print('Saving predictions on test images to results folder in the current directory')\n","            for i in np.arange(imgs_test_predictions.shape[0]):\n","              \n","              img_test_prediction = array_to_img(imgs_test_predictions[i,:,:])\n","              filename_start_index = self.test_imgs_list[i].rfind('/')\n","\n","              img_test_prediction.save(self.data_dir + 'results/%s' %(self.test_imgs_list[i][filename_start_index+1:]))\n","\n","if __name__ == '__main__':\n","\n","    # Make an object of myUnet class\n","    myunet = myUnet()\n","\n","    # Train and test the U-Net network as needed\n","    myunet.train_and_test()\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["------------------------------\n","Training on EC microscopy images\n","------------------------------\n","Loading training data (EC cells in microscopy images)\n","Found 32 images belonging to 1 classes.\n","Found 32 images belonging to 1 classes.\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/14', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/23', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/22', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/20', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/21', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/15', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/16', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/24', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/18', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/28', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/25', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/17', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/27', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/30', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/19', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/1', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/31', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/32', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/29', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/3', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/8', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/6', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/7', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/10', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/26', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/9', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/11', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/2', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/13', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/4', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/5', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/images/all', '/12', '.bmp']\n","32 EC test images found\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/1', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/28', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/25', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/29', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/32', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/2', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/27', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/30', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/31', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/26', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/4', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/3', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/9', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/5', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/7', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/6', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/10', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/11', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/8', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/12', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/13', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/17', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/19', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/14', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/18', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/21', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/15', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/22', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/16', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/20', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/23', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/train/labels/all', '/24', '.bmp']\n","32 EC test images found\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/54', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/56', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/57', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/63', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/61', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/55', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/62', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/59', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/36', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/60', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/38', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/37', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/34', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/65', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/64', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/39', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/45', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/42', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/46', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/35', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/33', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/41', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/40', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/43', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/49', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/51', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/50', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/52', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/47', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/44', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/53', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/48', '.bmp']\n","['/content/drive/My Drive/Colab Notebooks/test/images', '/58', '.bmp']\n","33 EC test images found\n","Loaded training data (EC cells in microscopy images) \n","\n","Loading network architecture\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 480, 320, 1)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 480, 320, 128 1280        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 480, 320, 128 147584      conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 240, 160, 128 0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 240, 160, 256 295168      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 240, 160, 256 590080      conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 120, 80, 256) 0           conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 120, 80, 512) 1180160     max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 120, 80, 512) 2359808     conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 120, 80, 512) 0           conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 60, 40, 512)  0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 60, 40, 1024) 4719616     max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 60, 40, 1024) 9438208     conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 60, 40, 1024) 0           conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 120, 80, 1024 0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 120, 80, 512) 2097664     up_sampling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 120, 80, 1024 0           dropout_1[0][0]                  \n","                                                                 conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 120, 80, 512) 4719104     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 120, 80, 512) 2359808     conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","up_sampling2d_2 (UpSampling2D)  (None, 240, 160, 512 0           conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 240, 160, 256 524544      up_sampling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 240, 160, 512 0           conv2d_4[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 240, 160, 256 1179904     concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 240, 160, 256 590080      conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","up_sampling2d_3 (UpSampling2D)  (None, 480, 320, 256 0           conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 480, 320, 128 131200      up_sampling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 480, 320, 256 0           conv2d_2[0][0]                   \n","                                                                 conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 480, 320, 128 295040      concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 480, 320, 128 147584      conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 480, 320, 2)  2306        conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 480, 320, 1)  3           conv2d_18[0][0]                  \n","==================================================================================================\n","Total params: 30,779,141\n","Trainable params: 30,779,141\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Loaded network architecture \n","\n","Fitting model to train dataset\n","Epoch 1/20\n","32/32 [==============================] - 1737s 54s/step - loss: 0.1661 - acc: 0.9241\n","\n","Epoch 00001: loss improved from inf to 0.16611, saving model to /content/drive/My Drive/Colab Notebooks/unet_train.hdf5\n","Epoch 2/20\n","32/32 [==============================] - 1746s 55s/step - loss: 0.1645 - acc: 0.9520\n","\n","Epoch 00002: loss improved from 0.16611 to 0.16449, saving model to /content/drive/My Drive/Colab Notebooks/unet_train.hdf5\n","Epoch 3/20\n","20/32 [=================>............] - ETA: 10:51 - loss: 0.1645 - acc: 0.9515"],"name":"stdout"}]}]}